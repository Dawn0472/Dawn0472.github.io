{"pages":[{"title":"archives","text":"","link":"/archives/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"關於我","text":"建立部落格初衷一名轉職後的軟體工程師 上一份工作結束後，開始整理過往的工作筆記與心得，架設部落格原因如下： 紀錄所思、所學、所做，提醒自己不忘成為更好的模樣 一路成長的路上受到很多人的照顧，不管是網路上大神文章的分享抑或是生活中他人實作經驗的教導與分享都令人收穫良多，也希望回饋他人 藉由文章紀錄曾經實作過的想法與作品，別讓時間衝淡記憶導致未來開發必須再從頭來過 部落格定位主要寫寫實作上的經驗，偶爾會夾帶對於生活工作上的一些想法，等實際文章累積到一定數量再重新整理風格 工作經歷 四年後端開發經驗 主力為Python，也利用Flask框架寫過網站，實作上搭配網頁開發三傑HTML、CSS、JavaScript 熟悉Requests與Scrapy爬蟲 參與並實作ＭＬ＆ＤＬ專案 熟悉finereport視覺化 搭配ＶＭ與Ｄocker建構服務 搭配MSSQL與MySQL資料庫 熟悉Git等開發工具 熟悉Crontab、Airflow等排程服務 部落格風格部落格風格參考HuliHexo的minos主題網路上分享的文章並不多，未來會以此部落格為模版寫一篇架構網站教學 文章聲明文章參考來源會附在文末，若有任何轉載需求請標註來源","link":"/about/index.html"}],"posts":[{"title":"Airflow with docker compose (airflow+mysql+rabbitmq) 分散式環境","text":"主機環境 兩台主機環境一致 操作系统: Ubuntu 16.04.7 LTS 内核版本: Linux 4.15.0-142-generic docker-compose版本: v1.29.1 叢集環境 管理對應目錄配置對應目錄，當啟用多個service可便於管理 MySQL配置文件: 放在/data/mysql airflow數據目錄: 放在/data/airflow 於serviceＡ、serviceＢ皆設定對應container目錄，並設定權限12$ mkdir -p ./data/airflow/dags ./logs/airflow ./data/airflow/plugins$ echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env serviceＡ(docker-compose.yml) 參照檔案建置master，修改yml: airflow單機版建構 github參考: https://github.com/Dawn0472/docker-airflow/tree/main/分散式/serverA MySQL 修改MySQL配置volumes對應路徑 x-airflow-common 環境 新增extra_hosts，設定hostname對應ip位置 修改對應volumes 1234567891011121314151617181920212223242526272829303132333435x-airflow-common: &amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.3} # build: . environment: &amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:worker@mysql/airflow # 變更為mysql連線方式 AIRFLOW__CELERY__RESULT_BACKEND: db+mysql://airflow:worker@mysql/airflow # 變更為mysql連線方式 AIRFLOW__CELERY__BROKER_URL: amqp://worker:worker@rabbitmq:5672// # 變更為rabbitmq連線方式 AIRFLOW__CORE__FERNET_KEY: '' AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' AIRFLOW__CORE__LOAD_EXAMPLES: 'true' AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth' _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} AIRFLOW__CORE__PARALLELISM: 64 AIRFLOW__CORE__DAG_CONCURRENCY: 32 AIRFLOW__SCHEDULER__PARSING_PROCESSES: 4 extra_hosts: - \"host-01:192.168.x.xx\" # worker hostname : ip - \"host-02:192.168.x.xx\" volumes: # 修改對應目錄 - ./data/airflow/dags:/opt/airflow/dags - ./logs/airflow:/opt/airflow/logs - ./data/airflow/plugins:/opt/airflow/plugins user: \"${AIRFLOW_UID:-50000}:0\" depends_on: &amp;airflow-common-depends-on rabbitmq: # 設置rabbitmq service名稱 condition: service_healthy mysql: # 設置mysql service名稱 condition: service_healthy airflow-init 刪除自建目錄command 12mkdir -p /sources/logs /sources/dags /sources/pluginschown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins} serviceＢ(docker-compose_worker.yml) 參照serviceＡ docker-compose.yml於services中刪除rabbitmq、mysql 需建置webserver，因webserver啟動時會啟動子服務log service，建置webserver才能於網頁中看到worker的log github參考: https://github.com/Dawn0472/docker-airflow/tree/main/分散式/serverB worker 刪除depends_on內容 啟用log server port 12345678910111213141516171819202122airflow-worker: &lt;&lt;: *airflow-common hostname: host-02 # 設定host_name command: celery worker ports: - \"8793:8793\" # 啟用log server port healthcheck: test: - \"CMD-SHELL\" - 'celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"' interval: 10s timeout: 10s retries: 5 environment: &lt;&lt;: *airflow-common-env # Required to handle warm shutdown of the celery workers properly # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation DUMB_INIT_SETSID: \"0\" restart: always depends_on: airflow-init: condition: service_completed_successfully x-airflow-common 環境 修改連接service 刪除depends_on內容 1234567891011121314151617181920212223242526272829x-airflow-common: &amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.3} # build: . environment: &amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:worker@192.168.x.xx:3305/airflow # 連接serverA mysql AIRFLOW__CELERY__RESULT_BACKEND: db+mysql://airflow:worker@192.168.x.xx:3305/airflow # 連接serverA mysql AIRFLOW__CELERY__BROKER_URL: amqp://worker:worker@192.168.x.xx:5672// # 連接serverA rabbitmq AIRFLOW__CORE__FERNET_KEY: '' AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' AIRFLOW__CORE__LOAD_EXAMPLES: 'true' AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth' _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} AIRFLOW__CORE__PARALLELISM: 64 AIRFLOW__CORE__DAG_CONCURRENCY: 32 AIRFLOW__SCHEDULER__PARSING_PROCESSES: 4 extra_hosts: - \"host-01:192.168.x.xx\" # worker hostname : ip - \"host-02:192.168.x.xx\" volumes: # 修改對應目錄 - ./data/airflow/dags:/opt/airflow/dags - ./logs/airflow:/opt/airflow/logs - ./data/airflow/plugins:/opt/airflow/plugins user: \"${AIRFLOW_UID:-50000}:0\" 啟動docker-compose 12$ docker-compose -f docker-compose_worker.yml up airflow-init # 初始化DB$ docker-compose -f docker-compose_worker.yml up -d # 創建airflow container airflow啟動畫面 flower啟動畫面 RabbitMQ啟動畫面 數據同步 讓server間數據同步，當scheduler進行排程調度時才不會發生找不到文件而導致無法運行排程的錯誤發生 數據同步有兩種方式 只更新一台server，讓所有server數據同步 只更新一台server，讓其他server對應更新數據的server位置 本次利用lsyncd實現多台server同步部屬，免密碼ssh登入 serverＡ lsyncd下載安裝:http://archive.ubuntu.com/ubuntu/pool/universe/l/lsyncd/lsyncd_2.2.3-1_amd64.deb 1$ sudo dpkg -i lsyncd_2.2.3-1_amd64.deb 檢查lsyncd版本 啟動lsyncd 12$ systemctl start lsyncd$ systemctl status lsyncd 建置lsyncd設定檔，path=/etc/lsyncd/lsyncd.conf.lua 1$ sudo vim /etc/lsyncd/lsyncd.conf.lua # 設定連接內容 配置節點之間通過公鑰連接 airflow-sync:私鑰 airflow-sync.pub:公鑰 1$ ssh-keygen -t rsa -C \"airflow-sync\" -b 4096 #生成名稱為airflow-sync的密鑰 將pub 公鑰內容複製到遠端serviceＢ上，遠端serviceＢ使用者目錄中.ssh/authorized_keys中會有對應公鑰內容 1$ ssh-copy-id -i ~/.ssh/airflow-sync.pub user@192.168.x.xx 放置測試文件於serviceＡ 路徑:/data/airflow/dags/sample-dag.py 列出hostname，以便測試是否task在不同機器上運行 12345678910111213141516171819202122232425262728from airflow import DAGfrom airflow.operators.bash import BashOperatorfrom datetime import datetime, timedeltadefault_args = { 'owner': 'Dawn', 'retries': 2, 'retry_delay': timedelta(minutes=1) }with DAG('dist_example', start_date=datetime(2022, 5, 13, 16, 34), schedule_interval=\"*/10 * * * *\", ) as dag: create_command = 'echo $(hostname)' t1 = BashOperator( task_id='task_for_q1', bash_command=create_command, dag=dag ) t2 = BashOperator( task_id= 'task_for_q2', bash_command=create_command, dag=dag ) t1 &gt;&gt; t2 於airflow運行 host-01 host-02 參考來源 Airflow2.2.3 + Celery + MySQL 8构建一个健壮的分布式调度集群:https://www.51cto.com/article/698027.html How to Scale-out Apache Airflow 2.0 with Redis and Celery:https://medium.com/codex/how-to-scale-out-apache-airflow-2-0-with-redis-and-celery-3e668e003b5c lsyncd官網：https://github.com/lsyncd/lsyncd Lsyncd搭建同步镜像-用Lsyncd实现本地和远程服务器之间实时同步:https://wzfou.com/lsyncd/","link":"/articles/2022/06/08/Airflow-with-docker-compose-airflow-mysql-rabbitmq-%E5%88%86%E6%95%A3%E5%BC%8F%E7%92%B0%E5%A2%83/"},{"title":"Airflow with docker compose (airflow+mysql+rabbitmq) 單機","text":"主機環境 操作系统: Ubuntu 16.04.7 LTS 内核版本: Linux 4.15.0-142-generic docker-compose版本: v1.29.1 docker-compose.yml 官方docker-compose.yaml提供版本為airflow+redis+postgres，因此需修改docker-compose.yml內容 MySQL airflow支援版本為5.7、8 因最終希望建立可彈性擴展的airflow環境，官方推薦MySQL 8+ 設置MySQL帳戶密碼 若本機已安裝MySQL，需修改本機對應port 1234567891011121314151617181920mysql: image: mysql:8.0.27 # 下載MySQL版本為8+以上 ports: - \"3305:3306\" # 區別本機MySQL port,修改對應port:3305 environment: MYSQL_ROOT_PASSWORD: a12345 # MySQL root帳密 MYSQL_USER: airflow # airflow 於DB中的帳號 MYSQL_PASSWORD: worker # airflow 於DB中的密碼 MYSQL_DATABASE: airflow # airflow 設定檔存放的DB名稱 command: [\"mysqld\",\"--default-authentication-plugin=mysql_native_password\",\"--collation-server=utf8mb4_general_ci\",\"--character-set-server=utf8mb4\"] volumes: - /app/mysqldata8:/var/lib/mysql # MySQL數據 - /app/my.cnf:/etc/my.cnf # MySQL設定檔 healthcheck: test: mysql --user=$$MYSQL_USER --password=$$MYSQL_PASSWORD -e 'SHOW DATABASES;' # healthcheck command interval: 5s retries: 5 restart: always cap_add: - SYS_NICE rabbitmq image下載management版本，可看監控畫面 123456789101112131415rabbitmq: image: rabbitmq:3-management-alpine environment: - RABBITMQ_DEFAULT_USER=worker # rabbitmq的使用者帳戶 - RABBITMQ_DEFAULT_PASS=worker # rabbitmq的使用者密碼 ports: - \"5672:5672\" # container 對應port - \"15672:15672\" # 監控畫面port healthcheck: test: rabbitmq-diagnostics -q ping interval: 5s timeout: 30s retries: 50 restart: always worker 設定host_name，以便於flower中辨別worker 123456789101112131415161718192021airflow-worker: &lt;&lt;: *airflow-common hostname: host-01 # 設定host_name command: celery worker healthcheck: test: - \"CMD-SHELL\" - 'celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"' interval: 10s timeout: 10s retries: 5 environment: &lt;&lt;: *airflow-common-env # Required to handle warm shutdown of the celery workers properly # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation DUMB_INIT_SETSID: \"0\" restart: always depends_on: &lt;&lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully x-airflow-common 環境 image下載版本2.0以上，本範例選擇airflow2.2.3 變更sevice連接方式 變更sevice建置名稱 123456789101112131415161718192021222324252627282930313233x-airflow-common: &amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.3} # build: . environment: &amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:worker@mysql/airflow # 變更為mysql連線方式 AIRFLOW__CELERY__RESULT_BACKEND: db+mysql://airflow:worker@mysql/airflow # 變更為mysql連線方式 AIRFLOW__CELERY__BROKER_URL: amqp://worker:worker@rabbitmq:5672// # 變更為rabbitmq連線方式 AIRFLOW__CORE__FERNET_KEY: '' AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' AIRFLOW__CORE__LOAD_EXAMPLES: 'true' AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth' _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} AIRFLOW__CORE__PARALLELISM: 64 AIRFLOW__CORE__DAG_CONCURRENCY: 32 AIRFLOW__SCHEDULER__PARSING_PROCESSES: 4 volumes: - ./dags:/opt/airflow/dags - ./logs:/opt/airflow/logs - ./plugins:/opt/airflow/plugins user: \"${AIRFLOW_UID:-50000}:0\" depends_on: &amp;airflow-common-depends-on rabbitmq: # 設置rabbitmq service名稱 condition: service_healthy mysql: # 設置mysql service名稱 condition: service_healthy 設定本機對應container目錄1$ mkdir -p ./dags ./logs ./plugins 設定airflow權限 確保AIRFLOW_UID為一般用戶UID 確保用戶擁有對應container目錄權限 1$ echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env 啟動docker-co mpose12$ docker-compose up airflow-init # 初始化DB$ docker-compose up -d # 創建airflow container 啟動後畫面 container status: healthy 當container status為unhealthy，可參照docker logs &lt;containerID&gt; 修改bugs 完整docker-compose.yml可參考https://github.com/Dawn0472/docker-airflow/tree/main/單機 參考來源 Airflow 2.2.3 容器化安装:https://mp.weixin.qq.com/s/VncpyXcTtlvnDkFrsAZ5lQ docker-compose healthcheck for rabbitMQ:https://devops.stackexchange.com/questions/12092/docker-compose-healthcheck-for-rabbitmq","link":"/articles/2022/06/07/Airflow-with-docker-compose-airflow-mysql-rabbitmq-%E5%96%AE%E6%A9%9F/"}],"tags":[{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"airflow","slug":"airflow","link":"/tags/airflow/"}],"categories":[{"name":"維運","slug":"維運","link":"/categories/%E7%B6%AD%E9%81%8B/"}]}