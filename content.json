{"pages":[{"title":"文章列表","text":"","link":"/archives/index.html"},{"title":"分類","text":"","link":"/categories/index.html"},{"title":"關於我","text":"建立部落格初衷一名轉職後的軟體工程師 上一份工作結束後，開始整理過往的工作筆記與心得，架設部落格原因如下： 紀錄所思、所學、所做，提醒自己不忘成為更好的模樣 一路成長的路上受到很多人的照顧，不管是網路上大神文章的分享抑或是生活中他人實作經驗的教導與分享都令人收穫良多，也希望回饋他人 藉由文章紀錄曾經實作過的想法與作品，別讓時間衝淡記憶導致未來開發必須再從頭來過 部落格定位主要寫寫實作上的經驗，偶爾會夾帶對於生活工作上的一些想法，等實際文章累積到一定數量再重新整理風格 工作經歷 四年後端開發經驗 主力為Python，也利用Flask框架寫過網站，實作上搭配網頁開發三傑HTML、CSS、JavaScript 熟悉Requests與Scrapy爬蟲 參與並實作ＭＬ＆ＤＬ專案 熟悉finereport視覺化 搭配ＶＭ與Ｄocker建構服務 搭配MSSQL與MySQL資料庫 熟悉Git等開發工具 熟悉Crontab、Airflow等排程服務 部落格風格部落格風格參考HuliHexo的minos主題網路上分享的文章並不多，未來會以此部落格為模版寫一篇架構網站教學 文章聲明文章參考來源會附在文中或文末，若有任何轉載需求請標註來源","link":"/about/index.html"}],"posts":[{"title":"Airflow with docker compose (airflow+mysql+rabbitmq) 分散式環境","text":"主機環境 兩台主機環境一致 操作系统: Ubuntu 16.04.7 LTS 内核版本: Linux 4.15.0-142-generic docker-compose版本: v1.29.1 叢集環境 管理對應目錄配置對應目錄，當啟用多個service可便於管理 MySQL配置文件: 放在/data/mysql airflow數據目錄: 放在/data/airflow 於serviceＡ、serviceＢ皆設定對應container目錄，並設定權限12$ mkdir -p ./data/airflow/dags ./logs/airflow ./data/airflow/plugins$ echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env serviceＡ(docker-compose.yml) 參照檔案建置master，修改yml: airflow單機版建構 github參考: https://github.com/Dawn0472/docker-airflow/tree/main/分散式/serverA MySQL 修改MySQL配置volumes對應路徑 x-airflow-common 環境 新增extra_hosts，設定hostname對應ip位置 修改對應volumes 1234567891011121314151617181920212223242526272829303132333435x-airflow-common: &amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.3} # build: . environment: &amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:worker@mysql/airflow # 變更為mysql連線方式 AIRFLOW__CELERY__RESULT_BACKEND: db+mysql://airflow:worker@mysql/airflow # 變更為mysql連線方式 AIRFLOW__CELERY__BROKER_URL: amqp://worker:worker@rabbitmq:5672// # 變更為rabbitmq連線方式 AIRFLOW__CORE__FERNET_KEY: '' AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' AIRFLOW__CORE__LOAD_EXAMPLES: 'true' AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth' _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} AIRFLOW__CORE__PARALLELISM: 64 AIRFLOW__CORE__DAG_CONCURRENCY: 32 AIRFLOW__SCHEDULER__PARSING_PROCESSES: 4 extra_hosts: - \"host-01:192.168.x.xx\" # worker hostname : ip - \"host-02:192.168.x.xx\" volumes: # 修改對應目錄 - ./data/airflow/dags:/opt/airflow/dags - ./logs/airflow:/opt/airflow/logs - ./data/airflow/plugins:/opt/airflow/plugins user: \"${AIRFLOW_UID:-50000}:0\" depends_on: &amp;airflow-common-depends-on rabbitmq: # 設置rabbitmq service名稱 condition: service_healthy mysql: # 設置mysql service名稱 condition: service_healthy airflow-init 刪除自建目錄command 12mkdir -p /sources/logs /sources/dags /sources/pluginschown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins} serviceＢ(docker-compose_worker.yml) 參照serviceＡ docker-compose.yml於services中刪除rabbitmq、mysql 需建置webserver，因webserver啟動時會啟動子服務log service，建置webserver才能於網頁中看到worker的log github參考: https://github.com/Dawn0472/docker-airflow/tree/main/分散式/serverB worker 刪除depends_on內容 啟用log server port 12345678910111213141516171819202122airflow-worker: &lt;&lt;: *airflow-common hostname: host-02 # 設定host_name command: celery worker ports: - \"8793:8793\" # 啟用log server port healthcheck: test: - \"CMD-SHELL\" - 'celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"' interval: 10s timeout: 10s retries: 5 environment: &lt;&lt;: *airflow-common-env # Required to handle warm shutdown of the celery workers properly # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation DUMB_INIT_SETSID: \"0\" restart: always depends_on: airflow-init: condition: service_completed_successfully x-airflow-common 環境 修改連接service 刪除depends_on內容 1234567891011121314151617181920212223242526272829x-airflow-common: &amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.3} # build: . environment: &amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:worker@192.168.x.xx:3305/airflow # 連接serverA mysql AIRFLOW__CELERY__RESULT_BACKEND: db+mysql://airflow:worker@192.168.x.xx:3305/airflow # 連接serverA mysql AIRFLOW__CELERY__BROKER_URL: amqp://worker:worker@192.168.x.xx:5672// # 連接serverA rabbitmq AIRFLOW__CORE__FERNET_KEY: '' AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' AIRFLOW__CORE__LOAD_EXAMPLES: 'true' AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth' _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} AIRFLOW__CORE__PARALLELISM: 64 AIRFLOW__CORE__DAG_CONCURRENCY: 32 AIRFLOW__SCHEDULER__PARSING_PROCESSES: 4 extra_hosts: - \"host-01:192.168.x.xx\" # worker hostname : ip - \"host-02:192.168.x.xx\" volumes: # 修改對應目錄 - ./data/airflow/dags:/opt/airflow/dags - ./logs/airflow:/opt/airflow/logs - ./data/airflow/plugins:/opt/airflow/plugins user: \"${AIRFLOW_UID:-50000}:0\" 啟動docker-compose 12$ docker-compose -f docker-compose_worker.yml up airflow-init # 初始化DB$ docker-compose -f docker-compose_worker.yml up -d # 創建airflow container airflow啟動畫面 flower啟動畫面 RabbitMQ啟動畫面 數據同步 讓server間數據同步，當scheduler進行排程調度時才不會發生找不到文件而導致無法運行排程的錯誤發生 數據同步有兩種方式 只更新一台server，讓所有server數據同步 只更新一台server，讓其他server對應更新數據的server位置 本次利用lsyncd實現多台server同步部屬，免密碼ssh登入 serverＡ lsyncd下載安裝:http://archive.ubuntu.com/ubuntu/pool/universe/l/lsyncd/lsyncd_2.2.3-1_amd64.deb 1$ sudo dpkg -i lsyncd_2.2.3-1_amd64.deb 檢查lsyncd版本 啟動lsyncd 12$ systemctl start lsyncd$ systemctl status lsyncd 建置lsyncd設定檔，path=/etc/lsyncd/lsyncd.conf.lua 1$ sudo vim /etc/lsyncd/lsyncd.conf.lua # 設定連接內容 配置節點之間通過公鑰連接 airflow-sync:私鑰 airflow-sync.pub:公鑰 1$ ssh-keygen -t rsa -C \"airflow-sync\" -b 4096 #生成名稱為airflow-sync的密鑰 將pub 公鑰內容複製到遠端serviceＢ上，遠端serviceＢ使用者目錄中.ssh/authorized_keys中會有對應公鑰內容 1$ ssh-copy-id -i ~/.ssh/airflow-sync.pub user@192.168.x.xx 放置測試文件於serviceＡ 路徑:/data/airflow/dags/sample-dag.py 列出hostname，以便測試是否task在不同機器上運行 12345678910111213141516171819202122232425262728from airflow import DAGfrom airflow.operators.bash import BashOperatorfrom datetime import datetime, timedeltadefault_args = { 'owner': 'Dawn', 'retries': 2, 'retry_delay': timedelta(minutes=1) }with DAG('dist_example', start_date=datetime(2022, 5, 13, 16, 34), schedule_interval=\"*/10 * * * *\", ) as dag: create_command = 'echo $(hostname)' t1 = BashOperator( task_id='task_for_q1', bash_command=create_command, dag=dag ) t2 = BashOperator( task_id= 'task_for_q2', bash_command=create_command, dag=dag ) t1 &gt;&gt; t2 於airflow運行 host-01 host-02 參考來源 Airflow2.2.3 + Celery + MySQL 8构建一个健壮的分布式调度集群:https://www.51cto.com/article/698027.html How to Scale-out Apache Airflow 2.0 with Redis and Celery:https://medium.com/codex/how-to-scale-out-apache-airflow-2-0-with-redis-and-celery-3e668e003b5c lsyncd官網：https://github.com/lsyncd/lsyncd Lsyncd搭建同步镜像-用Lsyncd实现本地和远程服务器之间实时同步:https://wzfou.com/lsyncd/","link":"/articles/2022/06/08/Airflow-with-docker-compose-airflow-mysql-rabbitmq-%E5%88%86%E6%95%A3%E5%BC%8F%E7%92%B0%E5%A2%83/"},{"title":"Airflow with docker compose (airflow+mysql+rabbitmq) 單機","text":"簡介 本篇針對docker-compose建構airflow+mysql+rabbitmq的環境 airflow基礎介紹可參考: 2022年，闲聊 Airflow 2.2 主機環境 操作系统: Ubuntu 16.04.7 LTS 内核版本: Linux 4.15.0-142-generic docker-compose版本: v1.29.1 docker-compose.yml 官方docker-compose.yaml提供版本為airflow+redis+postgres，因此需修改docker-compose.yml內容 MySQL airflow支援版本為5.7、8 因最終希望建立可彈性擴展的airflow環境，官方推薦MySQL 8+ 設置MySQL帳戶密碼 若本機已安裝MySQL，需修改本機對應port 1234567891011121314151617181920mysql: image: mysql:8.0.27 # 下載MySQL版本為8+以上 ports: - \"3305:3306\" # 區別本機MySQL port,修改對應port:3305 environment: MYSQL_ROOT_PASSWORD: a12345 # MySQL root帳密 MYSQL_USER: airflow # airflow 於DB中的帳號 MYSQL_PASSWORD: worker # airflow 於DB中的密碼 MYSQL_DATABASE: airflow # airflow 設定檔存放的DB名稱 command: [\"mysqld\",\"--default-authentication-plugin=mysql_native_password\",\"--collation-server=utf8mb4_general_ci\",\"--character-set-server=utf8mb4\"] volumes: - /app/mysqldata8:/var/lib/mysql # MySQL數據 - /app/my.cnf:/etc/my.cnf # MySQL設定檔 healthcheck: test: mysql --user=$$MYSQL_USER --password=$$MYSQL_PASSWORD -e 'SHOW DATABASES;' # healthcheck command interval: 5s retries: 5 restart: always cap_add: - SYS_NICE rabbitmq image下載management版本，可看監控畫面 123456789101112131415rabbitmq: image: rabbitmq:3-management-alpine environment: - RABBITMQ_DEFAULT_USER=worker # rabbitmq的使用者帳戶 - RABBITMQ_DEFAULT_PASS=worker # rabbitmq的使用者密碼 ports: - \"5672:5672\" # container 對應port - \"15672:15672\" # 監控畫面port healthcheck: test: rabbitmq-diagnostics -q ping interval: 5s timeout: 30s retries: 50 restart: always worker 設定host_name，以便於flower中辨別worker 123456789101112131415161718192021airflow-worker: &lt;&lt;: *airflow-common hostname: host-01 # 設定host_name command: celery worker healthcheck: test: - \"CMD-SHELL\" - 'celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"' interval: 10s timeout: 10s retries: 5 environment: &lt;&lt;: *airflow-common-env # Required to handle warm shutdown of the celery workers properly # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation DUMB_INIT_SETSID: \"0\" restart: always depends_on: &lt;&lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully x-airflow-common 環境 image下載版本2.0以上，本範例選擇airflow2.2.3 變更sevice連接方式 變更sevice建置名稱 123456789101112131415161718192021222324252627282930313233x-airflow-common: &amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.3} # build: . environment: &amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:worker@mysql/airflow # 變更為mysql連線方式 AIRFLOW__CELERY__RESULT_BACKEND: db+mysql://airflow:worker@mysql/airflow # 變更為mysql連線方式 AIRFLOW__CELERY__BROKER_URL: amqp://worker:worker@rabbitmq:5672// # 變更為rabbitmq連線方式 AIRFLOW__CORE__FERNET_KEY: '' AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' AIRFLOW__CORE__LOAD_EXAMPLES: 'true' AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth' _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} AIRFLOW__CORE__PARALLELISM: 64 AIRFLOW__CORE__DAG_CONCURRENCY: 32 AIRFLOW__SCHEDULER__PARSING_PROCESSES: 4 volumes: - ./dags:/opt/airflow/dags - ./logs:/opt/airflow/logs - ./plugins:/opt/airflow/plugins user: \"${AIRFLOW_UID:-50000}:0\" depends_on: &amp;airflow-common-depends-on rabbitmq: # 設置rabbitmq service名稱 condition: service_healthy mysql: # 設置mysql service名稱 condition: service_healthy 設定本機對應container目錄1$ mkdir -p ./dags ./logs ./plugins 設定airflow權限 確保AIRFLOW_UID為一般用戶UID 確保用戶擁有對應container目錄權限 1$ echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env 啟動docker-co mpose12$ docker-compose up airflow-init # 初始化DB$ docker-compose up -d # 創建airflow container 啟動後畫面 container status: healthy 當container status為unhealthy，可參照docker logs &lt;containerID&gt; 修改bugs 完整docker-compose.yml可參考https://github.com/Dawn0472/docker-airflow/tree/main/單機 參考來源 Airflow 2.2.3 容器化安装:https://mp.weixin.qq.com/s/VncpyXcTtlvnDkFrsAZ5lQ docker-compose healthcheck for rabbitMQ:https://devops.stackexchange.com/questions/12092/docker-compose-healthcheck-for-rabbitmq","link":"/articles/2022/06/07/Airflow-with-docker-compose-airflow-mysql-rabbitmq-%E5%96%AE%E6%A9%9F/"},{"title":"爬蟲基礎介紹-part1","text":"什麼是網路爬蟲爬蟲是一個程式 這個程式可以做什麼？ 可以模擬人類瀏覽網頁的行為，從中獲取所需的數據 比較口語化的說法是可以代替你自動從網站中取得資料，幾個優點如下，包括 不需要時時刻刻盯著各大網站的重要訊息，才能獲取第一手資料 從重複copy＆paste的過程中解放 獲得好幾個重要小幫手？ 爬蟲可以解決什麼問題爬蟲有很多的應用，從文章所述優點舉例說明 不需要時時刻刻盯著各大網站的重要訊息，才能獲取第一手資料 很多租屋族都希望可以抽到社會住宅，但是社會住宅的公告其實是不定期的，常常得知社會住宅釋出時，已經是公告抽籤結果了。此時爬蟲就可以自動幫你監看政府住宅單位的公告，只要政府一公佈消息就即時通知你，讓你能夠短時間內就上網填寫申請表單，不用再時不時上網查詢。 從重複copy＆paste的過程中解放 上班族應該都有經驗是有些報告或分析需要定期產出，部分資料需要擷取網站數據，再一筆筆複製貼到excel中最後產製結果。如果只有十幾筆很快即能完成，如果複製的數據來自各網頁只要有幾百筆的話，就會成為複製貼上的機器人，次數一多還可能貼錯，而爬蟲就能幫你從這樣呆板的行為中解放，讓你可以多出更多時間優化報告內容。 獲得好幾個重要小幫手 愛看演唱會的粉絲應該深有體驗，票開搶之際常常可能5分鐘之內售罄，此時簡直恨不得可以多10幾隻手從好幾台電腦下手開搶。而擁有爬蟲，就能在搶票時多好幾個小幫手協助自己自動搶票，成為搶票達人就不再是夢想，當然前提是機器效能夠高。 爬蟲小知識 - 爬蟲名稱由來網路爬蟲(web crawler)，也被稱蜘蛛，「web spider」 爬蟲，指的是在程式在網路中爬行，一步一步找到所需的數據 蜘蛛，指的是網際網路就像一張大網，程式就像蜘蛛一樣，會在這張網上到處爬行，最後將資料擷取下來 從網頁取得數據的流程無論是人為或是程式，當我們從網頁獲取資料時，大致的步驟如下：假設我們想知道博客來2021年度百大暢銷榜有哪些書，博客來2021年度百大暢銷榜 進入博客來網頁：每個我們使用的服務都有一個入口，在這裡我們先稱為網頁入口，即網頁連結，從瀏覽器中點擊百大暢銷榜的連結 資料的站點：資料的位置不一定放在入口網站的伺服器上，假設博客來除了入口伺服器外，後台有五台機器，排行榜的資料放在第三台機器上 要資料：當我們點擊連結的那一刻，瀏覽器就向博客來的伺服器索取資料 給資料：當取得資料後，伺服器開始回傳 結果：瀏覽器取得回傳後的資料，並呈現於設備上讓我們瀏覽 補充：至此從網頁獲取資料的過程就結束了，但在博客來後台每台機器都是可以相通的，也就是說今天資料不管放在第幾台機器上，只要有開放我們都能取得 如何入門爬蟲禮儀","link":"/articles/2022/06/12/%E7%88%AC%E8%9F%B2%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-part1/"}],"tags":[{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"airflow","slug":"airflow","link":"/tags/airflow/"},{"name":"爬蟲","slug":"爬蟲","link":"/tags/%E7%88%AC%E8%9F%B2/"},{"name":"python","slug":"python","link":"/tags/python/"}],"categories":[{"name":"維運","slug":"維運","link":"/categories/%E7%B6%AD%E9%81%8B/"},{"name":"爬蟲","slug":"爬蟲","link":"/categories/%E7%88%AC%E8%9F%B2/"}]}